#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""=================================================
@PROJECT_NAME: agentic_knowledge_system
@File    : test_gemini_client.py
@Author  : caixiongjiang
@Date    : 2026/1/5 18:00
@Function: 
    Gemini LLM Client æµ‹è¯•
    æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½ï¼šchatã€æµå¼ã€å¼‚æ­¥ã€å¤šæ¨¡æ€
@Modify History:
         
@Copyrightï¼šCopyright(c) 2024-2026. All Rights Reserved
=================================================="""

import sys
import asyncio
from pathlib import Path
import base64

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.client.llm import create_llm_client
from src.client.llm.types import LLMResponse


class TestGeminiClient:
    """Gemini LLM Client æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_results = []
        self.provider = "gemini"
        self.model_name = "gemini-2.5-flash"  # ä½¿ç”¨æœ€æ–°çš„å¿«é€Ÿæ¨¡å‹
    
    def log_result(self, test_name: str, passed: bool, message: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        result = f"{status} - {test_name}"
        if message:
            result += f": {message}"
        print(result)
        self.test_results.append((test_name, passed, message))
    
    def test_basic_chat(self):
        """æµ‹è¯• 1: åŸºç¡€å¯¹è¯"""
        print("\n" + "="*60)
        print("æµ‹è¯• 1: åŸºç¡€å¯¹è¯")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                temperature=0.7,
                max_tokens=200
            )
            
            response = client.generate(
                messages=[
                    {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ "}
                ]
            )
            
            assert isinstance(response, LLMResponse), "å“åº”ç±»å‹é”™è¯¯"
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            assert response.usage.total_tokens > 0, "Token ç»Ÿè®¡é”™è¯¯"
            
            print(f"\nğŸ“ å›ç­”: {response.content}")
            print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.total_tokens}")
            print(f"ğŸ¤– æ¨¡å‹: {response.model}")
            
            self.log_result("åŸºç¡€å¯¹è¯", True)
            
        except Exception as e:
            self.log_result("åŸºç¡€å¯¹è¯", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_system_instruction(self):
        """æµ‹è¯• 2: System Instructionï¼ˆGemini ç‰¹æœ‰ï¼‰"""
        print("\n" + "="*60)
        print("æµ‹è¯• 2: System Instruction")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                temperature=0.5,
                max_tokens=150
            )
            
            # Gemini å°† system message è½¬æ¢ä¸º systemInstruction
            response = client.generate(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè¯—äººï¼Œå›ç­”è¦æœ‰è¯—æ„å’ŒéŸµå¾‹"},
                    {"role": "user", "content": "æè¿°ä¸€ä¸‹æ˜¥å¤©"}
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            
            print(f"\nğŸ“ å›ç­”: {response.content}")
            print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.total_tokens}")
            print(f"ğŸ’¡ Gemini æˆåŠŸå¤„ç† system instruction")
            
            self.log_result("System Instruction", True)
            
        except Exception as e:
            self.log_result("System Instruction", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_streaming(self):
        """æµ‹è¯• 3: æµå¼è¾“å‡º"""
        print("\n" + "="*60)
        print("æµ‹è¯• 3: æµå¼è¾“å‡º")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=5000
            )
            
            print(f"\nğŸ“ æµå¼ç”Ÿæˆä¸­: ", end='', flush=True)
            
            full_content = ""
            chunk_count = 0
            
            for chunk in client.generate_stream(
                messages=[
                    {"role": "user", "content": "å¸®æˆ‘å†™ä¸€ä¸ªå…³äºæ·±åº¦å­¦ä¹ çš„è®ºæ–‡ï¼Œ3000å­—"}
                ]
            ):
                print(chunk.delta, end='', flush=True)
                full_content += chunk.delta
                chunk_count += 1
            
            print()  # æ¢è¡Œ
            
            assert full_content, "æµå¼å†…å®¹ä¸ºç©º"
            assert chunk_count > 0, "æœªæ”¶åˆ°ä»»ä½•å—"
            
            print(f"\nâœ… æµå¼è¾“å‡ºæˆåŠŸ")
            print(f"ğŸ“Š æ€»å…±æ”¶åˆ° {chunk_count} ä¸ªå—")
            print(f"ğŸ“ å®Œæ•´å†…å®¹: {full_content}")
            
            self.log_result("æµå¼è¾“å‡º", True)
            
        except Exception as e:
            self.log_result("æµå¼è¾“å‡º", False, str(e))
            print(f"\nâŒ é”™è¯¯: {e}")
    
    def test_async_call(self):
        """æµ‹è¯• 4: å¼‚æ­¥è°ƒç”¨"""
        print("\n" + "="*60)
        print("æµ‹è¯• 4: å¼‚æ­¥è°ƒç”¨")
        print("="*60)
        
        async def async_test():
            try:
                async with create_llm_client(
                    provider=self.provider,
                    model_name=self.model_name,
                    max_tokens=1000
                ) as client:
                    response = await client.agenerate(
                        messages=[
                            {"role": "user", "content": "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ"}
                        ]
                    )
                    
                    assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
                    
                    print(f"\nğŸ“ å›ç­”: {response.content}")
                    print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.total_tokens}")
                    print(f"âœ… å¼‚æ­¥è°ƒç”¨æˆåŠŸ")
                    
                    return True
                    
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                return False
        
        try:
            result = asyncio.run(async_test())
            self.log_result("å¼‚æ­¥è°ƒç”¨", result)
            
        except Exception as e:
            self.log_result("å¼‚æ­¥è°ƒç”¨", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_async_streaming(self):
        """æµ‹è¯• 5: å¼‚æ­¥æµå¼è¾“å‡º"""
        print("\n" + "="*60)
        print("æµ‹è¯• 5: å¼‚æ­¥æµå¼è¾“å‡º")
        print("="*60)
        
        async def async_stream_test():
            try:
                async with create_llm_client(
                    provider=self.provider,
                    model_name=self.model_name,
                    max_tokens=200
                ) as client:
                    print(f"\nğŸ“ å¼‚æ­¥æµå¼ç”Ÿæˆä¸­: ", end='', flush=True)
                    
                    full_content = ""
                    chunk_count = 0
                    
                    async for chunk in client.agenerate_stream(
                        messages=[
                            {"role": "user", "content": "ç”¨ä¸€æ®µè¯ä»‹ç»JavaScript"}
                        ]
                    ):
                        print(chunk.delta, end='', flush=True)
                        full_content += chunk.delta
                        chunk_count += 1
                    
                    print()  # æ¢è¡Œ
                    
                    assert full_content, "æµå¼å†…å®¹ä¸ºç©º"
                    assert chunk_count > 0, "æœªæ”¶åˆ°ä»»ä½•å—"
                    
                    print(f"\nâœ… å¼‚æ­¥æµå¼è¾“å‡ºæˆåŠŸ")
                    print(f"ğŸ“Š æ€»å…±æ”¶åˆ° {chunk_count} ä¸ªå—")
                    
                    return True
                    
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")
                return False
        
        try:
            result = asyncio.run(async_stream_test())
            self.log_result("å¼‚æ­¥æµå¼è¾“å‡º", result)
            
        except Exception as e:
            self.log_result("å¼‚æ­¥æµå¼è¾“å‡º", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_async_batch(self):
        """æµ‹è¯• 6: å¼‚æ­¥æ‰¹é‡å¹¶å‘"""
        print("\n" + "="*60)
        print("æµ‹è¯• 6: å¼‚æ­¥æ‰¹é‡å¹¶å‘")
        print("="*60)
        
        async def async_batch_test():
            try:
                async with create_llm_client(
                    provider=self.provider,
                    model_name=self.model_name,
                    max_tokens=100
                ) as client:
                    # æ‰¹é‡é—®é¢˜
                    questions = [
                        "1+1=?",
                        "2+2=?",
                        "3+3=?"
                    ]
                    
                    # å¹¶å‘è°ƒç”¨
                    tasks = [
                        client.agenerate(messages=[{"role": "user", "content": q}])
                        for q in questions
                    ]
                    
                    responses = await asyncio.gather(*tasks)
                    
                    print(f"\nâœ… æˆåŠŸå¤„ç† {len(responses)} ä¸ªè¯·æ±‚")
                    for i, (q, r) in enumerate(zip(questions, responses), 1):
                        print(f"   {i}. {q} â†’ {r.content[:50]}")
                    
                    assert len(responses) == len(questions), "å“åº”æ•°é‡ä¸åŒ¹é…"
                    return True
                    
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                return False
        
        try:
            result = asyncio.run(async_batch_test())
            self.log_result("å¼‚æ­¥æ‰¹é‡å¹¶å‘", result)
            
        except Exception as e:
            self.log_result("å¼‚æ­¥æ‰¹é‡å¹¶å‘", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_multimodal_base64(self):
        """æµ‹è¯• 7: å¤šæ¨¡æ€è¾“å…¥ - Base64 æ ¼å¼ï¼ˆGemini ç‰¹æœ‰ï¼‰"""
        print("\n" + "="*60)
        print("æµ‹è¯• 7: å¤šæ¨¡æ€è¾“å…¥ - Base64 æ ¼å¼")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                temperature=0.7,
                max_tokens=300
            )
            
            # è¯»å–æœ¬åœ°å›¾ç‰‡å¹¶è½¬æ¢ä¸º base64
            image_path = Path(__file__).parent.parent.parent.parent / "tmp_files" / "image" / "image.png"
            
            if not image_path.exists():
                print(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                self.log_result("å¤šæ¨¡æ€è¾“å…¥-Base64", False, "å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨")
                return
            
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")
            
            print(f"ğŸ“· å›¾ç‰‡è·¯å¾„: {image_path}")
            
            # å¤šæ¨¡æ€æ¶ˆæ¯
            response = client.generate(
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"},
                            {"type": "image_base64", "image_data": image_data}
                        ]
                    }
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            
            print(f"\nğŸ“ å›ç­”: {response.content}")
            print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.total_tokens}")
            print(f"ğŸ’¡ Gemini æˆåŠŸå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬ + Base64 å›¾ç‰‡ï¼‰")
            
            self.log_result("å¤šæ¨¡æ€è¾“å…¥-Base64", True)
            
        except Exception as e:
            self.log_result("å¤šæ¨¡æ€è¾“å…¥-Base64", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_multimodal_image_url(self):
        """æµ‹è¯• 8: å¤šæ¨¡æ€è¾“å…¥ - Image URL æ ¼å¼ï¼ˆGemini ç‰¹æœ‰ï¼‰"""
        print("\n" + "="*60)
        print("æµ‹è¯• 8: å¤šæ¨¡æ€è¾“å…¥ - Image URL æ ¼å¼")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                temperature=0.7,
                max_tokens=300
            )
            
            # ä½¿ç”¨å…¬å¼€çš„æµ‹è¯•å›¾ç‰‡ URL
            test_image_url = "https://www.deepseekss.com/wp-content/uploads/2025/03/82f29445f020ef4-1-png.webp"
            
            print(f"ğŸ“· å›¾ç‰‡ URL: {test_image_url}")
            
            # å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆä½¿ç”¨ image_url æ ¼å¼ï¼‰
            response = client.generate(
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"},
                            {"type": "image_url", "image_url": test_image_url}
                        ]
                    }
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            
            print(f"\nğŸ“ å›ç­”: {response.content}")
            print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.total_tokens}")
            print(f"ğŸ’¡ Gemini æˆåŠŸå¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬ + Image URLï¼‰")
            
            self.log_result("å¤šæ¨¡æ€è¾“å…¥-ImageURL", True)
            
        except Exception as e:
            self.log_result("å¤šæ¨¡æ€è¾“å…¥-ImageURL", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_context_manager(self):
        """æµ‹è¯• 9: ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆèµ„æºç®¡ç†ï¼‰"""
        print("\n" + "="*60)
        print("æµ‹è¯• 9: ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
        print("="*60)
        
        try:
            with create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=100
            ) as client:
                # å¤šæ¬¡è°ƒç”¨ï¼Œå¤ç”¨è¿æ¥
                response1 = client.generate(
                    messages=[{"role": "user", "content": "ä½ å¥½"}]
                )
                
                response2 = client.generate(
                    messages=[{"role": "user", "content": "å†è§"}]
                )
                
                assert response1.content, "ç¬¬ä¸€æ¬¡è°ƒç”¨å¤±è´¥"
                assert response2.content, "ç¬¬äºŒæ¬¡è°ƒç”¨å¤±è´¥"
                
                print(f"\nâœ… è°ƒç”¨ 1: {response1.content[:50]}")
                print(f"âœ… è°ƒç”¨ 2: {response2.content[:50]}")
                print(f"ğŸ’¡ è¿æ¥æ± è‡ªåŠ¨å¤ç”¨å’Œé‡Šæ”¾")
            
            self.log_result("ä¸Šä¸‹æ–‡ç®¡ç†å™¨", True)
            
        except Exception as e:
            self.log_result("ä¸Šä¸‹æ–‡ç®¡ç†å™¨", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸš€ Gemini LLM Client æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
        print("="*60)
        print(f"Provider: {self.provider}")
        print(f"Model: {self.model_name}")
        print("="*60)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        # self.test_basic_chat()
        # self.test_system_instruction()
        self.test_streaming()
        # self.test_async_call()
        # self.test_async_streaming()
        # self.test_async_batch()
        # self.test_multimodal_base64()
        # self.test_multimodal_image_url()
        # self.test_context_manager()
        
        # æ±‡æ€»ç»“æœ
        # print("\n" + "="*60)
        # print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        # print("="*60)
        
        # passed = sum(1 for _, p, _ in self.test_results if p)
        # total = len(self.test_results)
        
        # for name, passed_flag, message in self.test_results:
        #     status = "âœ…" if passed_flag else "âŒ"
        #     print(f"{status} {name}")
        #     if message and not passed_flag:
        #         print(f"   {message}")
        
        # print("\n" + "="*60)
        # print(f"æ€»è®¡: {passed}/{total} é€šè¿‡")
        # print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    tester = TestGeminiClient()
    tester.run_all_tests()


if __name__ == "__main__":
    main()
