#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""=================================================
@PROJECT_NAME: agentic_knowledge_system
@File    : test_openai_client.py
@Author  : caixiongjiang
@Date    : 2026/1/5 18:00
@Function: 
    OpenAI LLM Client æµ‹è¯•
    æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½ï¼šchatã€æµå¼ã€å¼‚æ­¥ã€ä¸Šä¸‹æ–‡ç®¡ç†
@Modify History:
         
@Copyrightï¼šCopyright(c) 2024-2026. All Rights Reserved
=================================================="""

# TODO: æµ‹è¯•OpenAIå®¢æˆ·ç«¯çš„æ€è€ƒèƒ½åŠ›

import sys
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.client.llm import create_llm_client
from src.client.llm.types import LLMResponse


class TestOpenAIClient:
    """OpenAI LLM Client æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_results = []
        self.provider = "openai"
        self.model_name = "google_cloud_gemini/gemini-2.0-flash"
    
    def log_result(self, test_name: str, passed: bool, message: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        result = f"{status} - {test_name}"
        if message:
            result += f": {message}"
        print(result)
        self.test_results.append((test_name, passed, message))
    
    def test_basic_chat(self):
        """æµ‹è¯• 1: åŸºç¡€å¯¹è¯"""
        print("\n" + "="*60)
        print("æµ‹è¯• 1: åŸºç¡€å¯¹è¯")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                temperature=0.7,
                max_tokens=200
            )
            
            response = client.generate(
                messages=[
                    {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"}
                ]
            )
            
            assert isinstance(response, LLMResponse), "å“åº”ç±»å‹é”™è¯¯"
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            assert response.usage.total_tokens > 0, "Token ç»Ÿè®¡é”™è¯¯"
            
            print(f"\nğŸ“ å›ç­”: {response.content}")
            print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.total_tokens}")
            print(f"ğŸ¤– æ¨¡å‹: {response.model}")
            
            self.log_result("åŸºç¡€å¯¹è¯", True)
            
        except Exception as e:
            self.log_result("åŸºç¡€å¯¹è¯", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_multi_turn_conversation(self):
        """æµ‹è¯• 2: å¤šè½®å¯¹è¯"""
        print("\n" + "="*60)
        print("æµ‹è¯• 2: å¤šè½®å¯¹è¯")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=150
            )
            
            response = client.generate(
                messages=[
                    {"role": "user", "content": "æˆ‘å«å¼ ä¸‰"},
                    {"role": "assistant", "content": "ä½ å¥½ï¼Œå¼ ä¸‰ï¼"},
                    {"role": "user", "content": "ä½ è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ"}
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            assert "å¼ ä¸‰" in response.content, "æœªè®°ä½ä¸Šä¸‹æ–‡"
            
            print(f"\nğŸ“ å›ç­”: {response.content}")
            print(f"âœ… æˆåŠŸè®°ä½ä¸Šä¸‹æ–‡ä¿¡æ¯")
            
            self.log_result("å¤šè½®å¯¹è¯", True)
            
        except Exception as e:
            self.log_result("å¤šè½®å¯¹è¯", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_streaming(self):
        """æµ‹è¯• 3: æµå¼è¾“å‡º"""
        print("\n" + "="*60)
        print("æµ‹è¯• 3: æµå¼è¾“å‡º")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=200
            )
            
            print(f"\nğŸ“ æµå¼ç”Ÿæˆä¸­: ", end='', flush=True)
            
            full_content = ""
            chunk_count = 0
            
            for chunk in client.generate_stream(
                messages=[
                    {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€"}
                ]
            ):
                print(chunk.delta, end='', flush=True)
                full_content += chunk.delta
                chunk_count += 1
            
            print()  # æ¢è¡Œ
            
            assert full_content, "æµå¼å†…å®¹ä¸ºç©º"
            assert chunk_count > 0, "æœªæ”¶åˆ°ä»»ä½•å—"
            
            print(f"\nâœ… æµå¼è¾“å‡ºæˆåŠŸ")
            print(f"ğŸ“Š æ€»å…±æ”¶åˆ° {chunk_count} ä¸ªå—")
            print(f"ğŸ“ å®Œæ•´å†…å®¹: {full_content}")
            
            self.log_result("æµå¼è¾“å‡º", True)
            
        except Exception as e:
            self.log_result("æµå¼è¾“å‡º", False, str(e))
            print(f"\nâŒ é”™è¯¯: {e}")
    
    def test_async_call(self):
        """æµ‹è¯• 4: å¼‚æ­¥è°ƒç”¨"""
        print("\n" + "="*60)
        print("æµ‹è¯• 4: å¼‚æ­¥è°ƒç”¨")
        print("="*60)
        
        async def async_test():
            try:
                async with create_llm_client(
                    provider=self.provider,
                    model_name=self.model_name,
                    max_tokens=200
                ) as client:
                    response = await client.agenerate(
                        messages=[
                            {"role": "user", "content": "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ"}
                        ]
                    )
                    
                    assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
                    
                    print(f"\nğŸ“ å›ç­”: {response.content}")
                    print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.total_tokens}")
                    print(f"âœ… å¼‚æ­¥è°ƒç”¨æˆåŠŸ")
                    
                    return True
                    
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                return False
        
        try:
            result = asyncio.run(async_test())
            self.log_result("å¼‚æ­¥è°ƒç”¨", result)
            
        except Exception as e:
            self.log_result("å¼‚æ­¥è°ƒç”¨", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_async_streaming(self):
        """æµ‹è¯• 5: å¼‚æ­¥æµå¼è¾“å‡º"""
        print("\n" + "="*60)
        print("æµ‹è¯• 5: å¼‚æ­¥æµå¼è¾“å‡º")
        print("="*60)
        
        async def async_stream_test():
            try:
                async with create_llm_client(
                    provider=self.provider,
                    model_name=self.model_name,
                    max_tokens=200
                ) as client:
                    print(f"\nğŸ“ å¼‚æ­¥æµå¼ç”Ÿæˆä¸­: ", end='', flush=True)
                    
                    full_content = ""
                    chunk_count = 0
                    
                    async for chunk in client.agenerate_stream(
                        messages=[
                            {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»JavaScript"}
                        ]
                    ):
                        print(chunk.delta, end='', flush=True)
                        full_content += chunk.delta
                        chunk_count += 1
                    
                    print()  # æ¢è¡Œ
                    
                    assert full_content, "æµå¼å†…å®¹ä¸ºç©º"
                    assert chunk_count > 0, "æœªæ”¶åˆ°ä»»ä½•å—"
                    
                    print(f"\nâœ… å¼‚æ­¥æµå¼è¾“å‡ºæˆåŠŸ")
                    print(f"ğŸ“Š æ€»å…±æ”¶åˆ° {chunk_count} ä¸ªå—")
                    
                    return True
                    
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")
                return False
        
        try:
            result = asyncio.run(async_stream_test())
            self.log_result("å¼‚æ­¥æµå¼è¾“å‡º", result)
            
        except Exception as e:
            self.log_result("å¼‚æ­¥æµå¼è¾“å‡º", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_async_batch(self):
        """æµ‹è¯• 6: å¼‚æ­¥æ‰¹é‡å¹¶å‘"""
        print("\n" + "="*60)
        print("æµ‹è¯• 6: å¼‚æ­¥æ‰¹é‡å¹¶å‘")
        print("="*60)
        
        async def async_batch_test():
            try:
                async with create_llm_client(
                    provider=self.provider,
                    model_name=self.model_name,
                    max_tokens=100
                ) as client:
                    questions = [
                        "Python æ˜¯ä»€ä¹ˆï¼Ÿ",
                        "JavaScript æ˜¯ä»€ä¹ˆï¼Ÿ",
                        "Java æ˜¯ä»€ä¹ˆï¼Ÿ"
                    ]
                    
                    tasks = [
                        client.agenerate(messages=[{"role": "user", "content": q}])
                        for q in questions
                    ]
                    
                    responses = await asyncio.gather(*tasks)
                    
                    print(f"\nâœ… æˆåŠŸå¤„ç† {len(responses)} ä¸ªè¯·æ±‚")
                    for i, (q, r) in enumerate(zip(questions, responses), 1):
                        print(f"   {i}. {q[:20]}... â†’ {r.content[:40]}...")
                    
                    assert len(responses) == len(questions), "å“åº”æ•°é‡ä¸åŒ¹é…"
                    return True
                    
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                return False
        
        try:
            result = asyncio.run(async_batch_test())
            self.log_result("å¼‚æ­¥æ‰¹é‡å¹¶å‘", result)
            
        except Exception as e:
            self.log_result("å¼‚æ­¥æ‰¹é‡å¹¶å‘", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_context_manager(self):
        """æµ‹è¯• 7: ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆèµ„æºç®¡ç†ï¼‰"""
        print("\n" + "="*60)
        print("æµ‹è¯• 7: ä¸Šä¸‹æ–‡ç®¡ç†å™¨")
        print("="*60)
        
        try:
            with create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=100
            ) as client:
                # å¤šæ¬¡è°ƒç”¨ï¼Œå¤ç”¨è¿æ¥
                response1 = client.generate(
                    messages=[{"role": "user", "content": "ä½ å¥½"}]
                )
                
                response2 = client.generate(
                    messages=[{"role": "user", "content": "å†è§"}]
                )
                
                assert response1.content, "ç¬¬ä¸€æ¬¡è°ƒç”¨å¤±è´¥"
                assert response2.content, "ç¬¬äºŒæ¬¡è°ƒç”¨å¤±è´¥"
                
                print(f"\nâœ… è°ƒç”¨ 1: {response1.content[:50]}")
                print(f"âœ… è°ƒç”¨ 2: {response2.content[:50]}")
                print(f"ğŸ’¡ è¿æ¥æ± è‡ªåŠ¨å¤ç”¨å’Œé‡Šæ”¾")
            
            self.log_result("ä¸Šä¸‹æ–‡ç®¡ç†å™¨", True)
            
        except Exception as e:
            self.log_result("ä¸Šä¸‹æ–‡ç®¡ç†å™¨", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸš€ OpenAI LLM Client æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
        print("="*60)
        print(f"Provider: {self.provider}")
        print(f"Model: {self.model_name}")
        print("="*60)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        self.test_basic_chat()
        self.test_multi_turn_conversation()
        self.test_streaming()
        self.test_async_call()
        self.test_async_streaming()
        self.test_async_batch()
        self.test_context_manager()
        
        # æ±‡æ€»ç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print("="*60)
        
        passed = sum(1 for _, p, _ in self.test_results if p)
        total = len(self.test_results)
        
        for name, passed_flag, message in self.test_results:
            status = "âœ…" if passed_flag else "âŒ"
            print(f"{status} {name}")
            if message and not passed_flag:
                print(f"   {message}")
        
        print("\n" + "="*60)
        print(f"æ€»è®¡: {passed}/{total} é€šè¿‡")
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    tester = TestOpenAIClient()
    tester.run_all_tests()


if __name__ == "__main__":
    main()
