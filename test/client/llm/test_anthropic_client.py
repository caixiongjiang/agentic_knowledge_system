#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""=================================================
@PROJECT_NAME: agentic_knowledge_system
@File    : test_anthropic_client.py
@Author  : caixiongjiang
@Date    : 2026/1/5 18:00
@Function: 
    Anthropic (Claude) LLM Client æµ‹è¯•
    æµ‹è¯• Claude ç‰¹æœ‰çš„ max_tokens å¿…å¡«å’Œ system message å¤„ç†
@Modify History:
         
@Copyrightï¼šCopyright(c) 2024-2026. All Rights Reserved
=================================================="""


# TODOï¼š æµ‹è¯•Anthropic å®¢æˆ·ç«¯

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.client.llm import create_llm_client
from src.client.llm.types import LLMResponse


class TestAnthropicClient:
    """Anthropic (Claude) LLM Client æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_results = []
        self.provider = "anthropic"
        self.model_name = "claude-3-5-sonnet-20241022"
    
    def log_result(self, test_name: str, passed: bool, message: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        result = f"{status} - {test_name}"
        if message:
            result += f": {message}"
        print(result)
        self.test_results.append((test_name, passed, message))
    
    def test_max_tokens_required(self):
        """æµ‹è¯• 1: max_tokens å¿…å¡«å‚æ•°"""
        print("\n" + "="*60)
        print("æµ‹è¯• 1: Claude max_tokens å¿…å¡«éªŒè¯")
        print("="*60)
        
        try:
            # æµ‹è¯•ä¸æä¾› max_tokensï¼ˆåº”è¯¥å¤±è´¥æˆ–ä½¿ç”¨é»˜è®¤å€¼ï¼‰
            try:
                client_no_max_tokens = create_llm_client(
                    provider=self.provider,
                    model_name=self.model_name,
                    temperature=0.7
                    # æ•…æ„ä¸æä¾› max_tokens
                )
                
                # å¦‚æœæ²¡æœ‰é»˜è®¤å€¼ï¼Œåº”è¯¥åœ¨ generate æ—¶å¤±è´¥
                response = client_no_max_tokens.generate(
                    messages=[{"role": "user", "content": "æµ‹è¯•"}]
                )
                
                # å¦‚æœæˆåŠŸäº†ï¼Œè¯´æ˜æœ‰é»˜è®¤å€¼ï¼ˆ4096ï¼‰
                print(f"âœ… æœªæä¾› max_tokensï¼Œä½¿ç”¨é»˜è®¤å€¼: {response.usage.completion_tokens} tokens")
                
            except ValueError as e:
                if "max_tokens" in str(e):
                    print(f"âœ… æ­£ç¡®æ£€æµ‹åˆ° max_tokens ç¼ºå¤±: {e}")
                else:
                    raise
            
            # æµ‹è¯•æä¾› max_tokensï¼ˆåº”è¯¥æˆåŠŸï¼‰
            client_with_max_tokens = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                temperature=0.7,
                max_tokens=500  # æ˜ç¡®æä¾›
            )
            
            response = client_with_max_tokens.generate(
                messages=[{"role": "user", "content": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            assert response.usage.completion_tokens <= 510, f"è¶…å‡ºé™åˆ¶: {response.usage.completion_tokens}"
            
            print(f"\nğŸ“ ç”Ÿæˆå†…å®¹: {response.content[:200]}...")
            print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.completion_tokens} / 500")
            
            self.log_result("max_tokens å¿…å¡«éªŒè¯", True)
            
        except Exception as e:
            self.log_result("max_tokens å¿…å¡«éªŒè¯", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_system_message_handling(self):
        """æµ‹è¯• 2: System Message å•ç‹¬å­—æ®µå¤„ç†"""
        print("\n" + "="*60)
        print("æµ‹è¯• 2: Claude System Message å¤„ç†")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=300,
                temperature=0.5
            )
            
            # Claude å°† system message æ”¾åœ¨å•ç‹¬çš„ "system" å­—æ®µ
            response = client.generate(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘æŠ€è®°è€…ï¼Œæ“…é•¿ç”¨ç®€æ´çš„è¯­è¨€è§£é‡Šå¤æ‚çš„æŠ€æœ¯æ¦‚å¿µ"},
                    {"role": "user", "content": "è§£é‡Šä¸€ä¸‹åŒºå—é“¾æŠ€æœ¯"}
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            
            print(f"\nğŸ“ ç”Ÿæˆå†…å®¹: {response.content[:300]}...")
            print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage.total_tokens}")
            print(f"ğŸ’¡ Claude æˆåŠŸå¤„ç† system message")
            
            self.log_result("System Message å¤„ç†", True)
            
        except Exception as e:
            self.log_result("System Message å¤„ç†", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_multiple_system_messages(self):
        """æµ‹è¯• 3: å¤šä¸ª System Message åˆå¹¶"""
        print("\n" + "="*60)
        print("æµ‹è¯• 3: Claude å¤šä¸ª System Message åˆå¹¶")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=300,
                temperature=0.5
            )
            
            # æä¾›å¤šä¸ª system messageï¼Œåº”è¯¥è¢«åˆå¹¶
            response = client.generate(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦è€å¸ˆ"},
                    {"role": "system", "content": "ä½ æ“…é•¿ç”¨ç®€å•çš„ä¾‹å­è§£é‡Šæ•°å­¦æ¦‚å¿µ"},
                    {"role": "user", "content": "ä»€ä¹ˆæ˜¯å¯¼æ•°ï¼Ÿ"}
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            
            print(f"\nğŸ“ ç”Ÿæˆå†…å®¹: {response.content[:300]}...")
            print(f"ğŸ’¡ å¤šä¸ª system message å·²åˆå¹¶")
            
            self.log_result("å¤šä¸ª System Message åˆå¹¶", True)
            
        except Exception as e:
            self.log_result("å¤šä¸ª System Message åˆå¹¶", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_message_alternation(self):
        """æµ‹è¯• 4: æ¶ˆæ¯äº¤æ›¿ï¼ˆuser/assistantï¼‰"""
        print("\n" + "="*60)
        print("æµ‹è¯• 4: Claude æ¶ˆæ¯äº¤æ›¿éªŒè¯")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=200
            )
            
            # æ­£ç¡®çš„äº¤æ›¿ï¼šuser -> assistant -> user
            response = client.generate(
                messages=[
                    {"role": "user", "content": "æˆ‘å«ææ˜"},
                    {"role": "assistant", "content": "ä½ å¥½ææ˜ï¼"},
                    {"role": "user", "content": "ä½ è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ"}
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            assert "ææ˜" in response.content, "æ¨¡å‹æœªè®°ä½ä¸Šä¸‹æ–‡"
            
            print(f"\nğŸ“ ç”Ÿæˆå†…å®¹: {response.content}")
            print(f"ğŸ’¡ æ¶ˆæ¯äº¤æ›¿æ­£ç¡®")
            
            self.log_result("æ¶ˆæ¯äº¤æ›¿éªŒè¯", True)
            
        except Exception as e:
            self.log_result("æ¶ˆæ¯äº¤æ›¿éªŒè¯", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_long_context(self):
        """æµ‹è¯• 5: é•¿ä¸Šä¸‹æ–‡å¤„ç†"""
        print("\n" + "="*60)
        print("æµ‹è¯• 5: Claude é•¿ä¸Šä¸‹æ–‡å¤„ç†")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=500,
                temperature=0.5
            )
            
            # Claude 3.5 Sonnet æ”¯æŒ 200K ä¸Šä¸‹æ–‡
            long_text = "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚" * 100
            
            response = client.generate(
                messages=[
                    {
                        "role": "user",
                        "content": f"ä»¥ä¸‹æ˜¯ä¸€æ®µæ–‡æœ¬ï¼š\n\n{long_text}\n\nè¯·ç”¨ä¸€å¥è¯æ€»ç»“è¿™æ®µæ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹ã€‚"
                    }
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            
            print(f"\nğŸ“ è¾“å…¥é•¿åº¦: {len(long_text)} å­—ç¬¦")
            print(f"ğŸ“ è¾“å…¥ Tokens: {response.usage.prompt_tokens}")
            print(f"ğŸ“ ç”Ÿæˆå†…å®¹: {response.content}")
            print(f"ğŸ’¡ Claude æˆåŠŸå¤„ç†é•¿ä¸Šä¸‹æ–‡")
            
            self.log_result("é•¿ä¸Šä¸‹æ–‡å¤„ç†", True)
            
        except Exception as e:
            self.log_result("é•¿ä¸Šä¸‹æ–‡å¤„ç†", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_temperature_range(self):
        """æµ‹è¯• 6: Temperature èŒƒå›´éªŒè¯"""
        print("\n" + "="*60)
        print("æµ‹è¯• 6: Claude Temperature èŒƒå›´")
        print("="*60)
        
        try:
            # Claude çš„ temperature èŒƒå›´æ˜¯ [0, 1]ï¼ˆä¸æ˜¯ [0, 2]ï¼‰
            
            # æµ‹è¯•åˆæ³•å€¼
            client_valid = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=100,
                temperature=0.5  # åˆæ³•
            )
            
            response_valid = client_valid.generate(
                messages=[{"role": "user", "content": "æµ‹è¯•"}]
            )
            
            print(f"âœ… temperature=0.5 åˆæ³•: {response_valid.content[:50]}...")
            
            # æµ‹è¯•éæ³•å€¼
            try:
                client_invalid = create_llm_client(
                    provider=self.provider,
                    model_name=self.model_name,
                    max_tokens=100,
                    temperature=1.5  # éæ³•ï¼ˆClaude åªæ”¯æŒ [0, 1]ï¼‰
                )
                
                response_invalid = client_invalid.generate(
                    messages=[{"role": "user", "content": "æµ‹è¯•"}]
                )
                
                print("âŒ åº”è¯¥æŠ›å‡º temperature èŒƒå›´é”™è¯¯")
                self.log_result("Temperature èŒƒå›´", False, "æœªæ£€æµ‹åˆ°éæ³• temperature")
                
            except ValueError as e:
                if "temperature" in str(e).lower():
                    print(f"âœ… æ­£ç¡®æ•è· temperature é”™è¯¯: {e}")
                    self.log_result("Temperature èŒƒå›´", True)
                else:
                    raise
            
        except Exception as e:
            self.log_result("Temperature èŒƒå›´", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_stop_sequences(self):
        """æµ‹è¯• 7: Stop Sequences"""
        print("\n" + "="*60)
        print("æµ‹è¯• 7: Claude Stop Sequences")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=500,
                stop_sequences=["ç»“æŸ", "END"]  # Claude ç‰¹æœ‰å‚æ•°
            )
            
            response = client.generate(
                messages=[
                    {"role": "user", "content": "åˆ—å‡º3ç§ç¼–ç¨‹è¯­è¨€ï¼Œæ¯ä¸ªåé¢å†™ä¸Š'ç»“æŸ'"}
                ]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            
            print(f"\nğŸ“ ç”Ÿæˆå†…å®¹: {response.content}")
            print(f"ğŸ Finish Reason: {response.finish_reason}")
            print(f"ğŸ’¡ Stop sequences é…ç½®æˆåŠŸ")
            
            self.log_result("Stop Sequences", True)
            
        except Exception as e:
            self.log_result("Stop Sequences", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_finish_reason_mapping(self):
        """æµ‹è¯• 8: Finish Reason æ˜ å°„"""
        print("\n" + "="*60)
        print("æµ‹è¯• 8: Claude Finish Reason æ˜ å°„")
        print("="*60)
        
        try:
            # æ­£å¸¸å®Œæˆ
            client_normal = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=1000
            )
            
            response_normal = client_normal.generate(
                messages=[{"role": "user", "content": "2+2=?"}]
            )
            
            print(f"\nâœ… æ­£å¸¸å®Œæˆ:")
            print(f"   Finish Reason: {response_normal.finish_reason}")
            print(f"   åº”è¯¥æ˜¯: 'stop'")
            assert response_normal.finish_reason == "stop", f"Finish reason é”™è¯¯: {response_normal.finish_reason}"
            
            # Token é™åˆ¶
            client_limited = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=20
            )
            
            response_limited = client_limited.generate(
                messages=[{"role": "user", "content": "è¯¦ç»†ä»‹ç»æ·±åº¦å­¦ä¹ çš„å‘å±•å†å²å’Œæœªæ¥å±•æœ›"}]
            )
            
            print(f"\nâš ï¸ Token é™åˆ¶:")
            print(f"   Finish Reason: {response_limited.finish_reason}")
            print(f"   åº”è¯¥æ˜¯: 'length'")
            assert response_limited.finish_reason == "length", f"Finish reason é”™è¯¯: {response_limited.finish_reason}"
            
            print(f"\nğŸ’¡ Claude Finish Reason æ˜ å°„æˆåŠŸ: end_turn â†’ stop, max_tokens â†’ length")
            
            self.log_result("Finish Reason æ˜ å°„", True)
            
        except Exception as e:
            self.log_result("Finish Reason æ˜ å°„", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_api_version_header(self):
        """æµ‹è¯• 9: API ç‰ˆæœ¬å¤´"""
        print("\n" + "="*60)
        print("æµ‹è¯• 9: Claude API ç‰ˆæœ¬å¤´")
        print("="*60)
        
        try:
            # Claude ä½¿ç”¨ç‰¹æ®Šçš„ API ç‰ˆæœ¬å¤´å’Œè®¤è¯æ–¹å¼
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=100
            )
            
            response = client.generate(
                messages=[{"role": "user", "content": "æµ‹è¯• API ç‰ˆæœ¬å¤´"}]
            )
            
            assert response.content, "å“åº”å†…å®¹ä¸ºç©º"
            
            print(f"\nğŸ“ ç”Ÿæˆå†…å®¹: {response.content}")
            print(f"ğŸ’¡ API ç‰ˆæœ¬å¤´é…ç½®æ­£ç¡®")
            print(f"   - x-api-key: [å·²é…ç½®]")
            print(f"   - anthropic-version: 2023-06-01")
            
            self.log_result("API ç‰ˆæœ¬å¤´", True)
            
        except Exception as e:
            self.log_result("API ç‰ˆæœ¬å¤´", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def test_response_structure(self):
        """æµ‹è¯• 10: å“åº”ç»“æ„å®Œæ•´æ€§"""
        print("\n" + "="*60)
        print("æµ‹è¯• 10: Claude å“åº”ç»“æ„")
        print("="*60)
        
        try:
            client = create_llm_client(
                provider=self.provider,
                model_name=self.model_name,
                max_tokens=200
            )
            
            response = client.generate(
                messages=[{"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ Claude"}]
            )
            
            # éªŒè¯å“åº”ç»“æ„
            assert isinstance(response, LLMResponse), "å“åº”ç±»å‹é”™è¯¯"
            assert response.content, "content ä¸ºç©º"
            assert response.usage, "usage ä¸ºç©º"
            assert response.model, "model ä¸ºç©º"
            assert response.finish_reason, "finish_reason ä¸ºç©º"
            assert response.raw_response, "raw_response ä¸ºç©º"
            
            # éªŒè¯ thinking å­—æ®µï¼ˆClaude ä¸æ”¯æŒï¼‰
            assert response.thinking is None, "Claude ä¸åº”æœ‰ thinking å­—æ®µ"
            assert response.usage.thinking_tokens is None, "Claude ä¸åº”ç»Ÿè®¡ thinking tokens"
            
            print(f"\nâœ… å“åº”ç»“æ„å®Œæ•´:")
            print(f"   - content: âœ… ({len(response.content)} å­—ç¬¦)")
            print(f"   - usage: âœ… ({response.usage.total_tokens} tokens)")
            print(f"   - model: âœ… ({response.model})")
            print(f"   - finish_reason: âœ… ({response.finish_reason})")
            print(f"   - thinking: âœ… (None - æ­£ç¡®)")
            
            self.log_result("å“åº”ç»“æ„", True)
            
        except Exception as e:
            self.log_result("å“åº”ç»“æ„", False, str(e))
            print(f"âŒ é”™è¯¯: {e}")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹ Anthropic (Claude) LLM Client æµ‹è¯•")
        print("="*60)
        print(f"Provider: {self.provider}")
        print(f"Model: {self.model_name}")
        print("ç‰¹æ€§: max_tokens å¿…å¡«ã€System Message å•ç‹¬å­—æ®µã€æ¸©åº¦èŒƒå›´ [0,1]")
        print("="*60)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        self.test_max_tokens_required()
        self.test_system_message_handling()
        self.test_multiple_system_messages()
        self.test_message_alternation()
        self.test_long_context()
        self.test_temperature_range()
        self.test_stop_sequences()
        self.test_finish_reason_mapping()
        self.test_api_version_header()
        self.test_response_structure()
        
        # æ±‡æ€»ç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print("="*60)
        
        passed = sum(1 for _, p, _ in self.test_results if p)
        total = len(self.test_results)
        
        for name, passed_flag, message in self.test_results:
            status = "âœ…" if passed_flag else "âŒ"
            print(f"{status} {name}")
            if message and not passed_flag:
                print(f"   {message}")
        
        print("\n" + "="*60)
        print(f"æ€»è®¡: {passed}/{total} é€šè¿‡")
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    tester = TestAnthropicClient()
    tester.run_all_tests()


if __name__ == "__main__":
    main()
