# LLM Client ä½¿ç”¨æŒ‡å—

**ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-05  
**çŠ¶æ€**: âœ… å·²å®Œæˆ

---

## ğŸ“‹ ç›®å½•

1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ”¯æŒçš„Provider](#æ”¯æŒçš„provider)
3. [åŸºæœ¬ç”¨æ³•](#åŸºæœ¬ç”¨æ³•)
4. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
5. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

1. **è®¾ç½®API Keyï¼ˆåœ¨.envæ–‡ä»¶ä¸­ï¼‰**

```bash
# DeepSeek
DEEPSEEK_API_KEY=sk-xxx

# OpenAI
OPENAI_API_KEY=sk-xxx

# Gemini
GEMINI_API_KEY=xxx

# Anthropic (Claude)
ANTHROPIC_API_KEY=sk-ant-xxx
```

2. **å®‰è£…ä¾èµ–**

```bash
pip install httpx pydantic loguru
```

### æœ€ç®€å•çš„ç¤ºä¾‹

```python
from src.client.llm import create_llm_client

# åˆ›å»ºå®¢æˆ·ç«¯
client = create_llm_client(
    provider="deepseek",
    model_name="deepseek-chat",
    temperature=0.0,
    max_tokens=1000
)

# å‘é€è¯·æ±‚
response = client.generate(
    messages=[
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"}
    ]
)

# è·å–ç»“æœ
print(response.content)  # å›ç­”å†…å®¹
print(response.usage.total_tokens)  # Tokenä½¿ç”¨
```

---

## ğŸŒ æ”¯æŒçš„Provider

| Provider | æ¨¡å‹ç¤ºä¾‹ | ç‰¹æ€§ | API Keyç¯å¢ƒå˜é‡ |
|----------|---------|------|----------------|
| **OpenAI** | gpt-4o, gpt-4-turbo | æ ‡å‡†æ ¼å¼ | OPENAI_API_KEY |
| **DeepSeek** | deepseek-chat, deepseek-reasoner | V3.2ï¼Œæ”¯æŒæ¨ç†æ¨¡å¼ | DEEPSEEK_API_KEY |
| **Gemini** | gemini-1.5-pro | å¤šæ¨¡æ€æ”¯æŒ | GEMINI_API_KEY |
| **Anthropic** | claude-3-5-sonnet | é•¿ä¸Šä¸‹æ–‡ | ANTHROPIC_API_KEY |

### DeepSeek-V3.2 æ¨¡å‹è¯´æ˜

- **deepseek-chat**: éæ€è€ƒæ¨¡å¼ï¼Œå¿«é€Ÿå“åº”ï¼Œé€‚åˆä¸€èˆ¬å¯¹è¯
- **deepseek-reasoner**: æ€è€ƒæ¨¡å¼ï¼Œé€‚åˆå¤æ‚æ¨ç†ä»»åŠ¡ï¼ˆæ¨èï¼‰

---

## ğŸ’¡ åŸºæœ¬ç”¨æ³•

### 1. ç›´æ¥ä½¿ç”¨ï¼ˆä¸´æ—¶æ¨¡å¼ï¼‰

```python
from src.client.llm import create_llm_client

client = create_llm_client("deepseek", "deepseek-chat")
response = client.generate(messages=[...])
# ä¸´æ—¶HTTPå®¢æˆ·ç«¯è‡ªåŠ¨å…³é—­ï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†
```

### 2. ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ¨èï¼Œæ‰¹é‡å¤„ç†ï¼‰

```python
with create_llm_client("deepseek", "deepseek-chat") as client:
    # å¤ç”¨è¿æ¥æ± ï¼Œæ€§èƒ½æ›´ä¼˜
    response1 = client.generate(messages=[...])
    response2 = client.generate(messages=[...])
    response3 = client.generate(messages=[...])
# è‡ªåŠ¨å…³é—­è¿æ¥æ± 
```

### 3. å¼‚æ­¥æ¨¡å¼

```python
import asyncio
from src.client.llm import create_llm_client

async def main():
    async with create_llm_client("deepseek", "deepseek-chat") as client:
        response = await client.agenerate(messages=[...])
        print(response.content)

asyncio.run(main())
```

---

## ğŸ”¥ é«˜çº§ç”¨æ³•

### 1. å¼‚æ­¥å¹¶å‘ï¼ˆé«˜æ€§èƒ½ï¼‰

```python
import asyncio
from src.client.llm import create_llm_client

async def process_batch(questions):
    """æ‰¹é‡å¤„ç†ï¼Œå¹¶å‘æ‰§è¡Œ"""
    async with create_llm_client("deepseek", "deepseek-chat") as client:
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [
            client.agenerate(messages=[{"role": "user", "content": q}])
            for q in questions
        ]
        
        # å¹¶å‘æ‰§è¡Œï¼ˆå¤ç”¨è¿æ¥æ± ï¼‰
        responses = await asyncio.gather(*tasks)
        
        return responses

# è¿è¡Œ
questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3", ...]
responses = asyncio.run(process_batch(questions))
```

**æ€§èƒ½å¯¹æ¯”ï¼š**
- ä¸²è¡Œå¤„ç†100ä¸ªè¯·æ±‚ï¼š~1000ç§’
- å¼‚æ­¥å¹¶å‘ï¼š~10ç§’ï¼ˆ100å€æå‡ï¼‰

### 2. æ§åˆ¶å¹¶å‘æ•°é‡ï¼ˆé¿å…è¿‡è½½ï¼‰

```python
import asyncio
from src.client.llm import create_llm_client

async def process_with_limit(questions, max_concurrent=10):
    """é™åˆ¶å¹¶å‘æ•°é‡"""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def limited_generate(client, msg):
        async with semaphore:
            return await client.agenerate(messages=msg)
    
    async with create_llm_client("deepseek", "deepseek-chat") as client:
        tasks = [limited_generate(client, [{"role": "user", "content": q}]) 
                 for q in questions]
        return await asyncio.gather(*tasks)

# 100ä¸ªè¯·æ±‚ï¼Œä½†åŒæ—¶åªæœ‰10ä¸ªåœ¨æ‰§è¡Œ
responses = asyncio.run(process_with_limit(questions, max_concurrent=10))
```

### 3. DeepSeek-V3.2 æ¨ç†æ¨¡å¼

```python
# æ–¹å¼1: ä½¿ç”¨ deepseek-reasonerï¼ˆæ¨èï¼‰
client = create_llm_client(
    provider="deepseek",
    model_name="deepseek-reasoner",  # V3.2 æ€è€ƒæ¨¡å¼
    temperature=0.0,
    max_tokens=500
)

response = client.generate(messages=[...])

# è®¿é—®æ¨ç†è¿‡ç¨‹
if response.thinking:
    print("æ¨ç†è¿‡ç¨‹:", response.thinking.reasoning)
    print("æ¨ç†Token:", response.thinking.tokens_used)

# æ–¹å¼2: ä½¿ç”¨ deepseek-chat + enable_thinkingï¼ˆç­‰ä»·ï¼‰
client = create_llm_client(
    provider="deepseek",
    model_name="deepseek-chat",
    enable_thinking=True  # å¯ç”¨æ¨ç†
)
```

**æ¨è**: ç›´æ¥ä½¿ç”¨ `deepseek-reasoner`ï¼Œè¯­ä¹‰æ›´æ¸…æ™°ã€‚

### 4. å¤šæ¨¡æ€ï¼ˆGeminiï¼‰

```python
client = create_llm_client("gemini", "gemini-1.5-pro")

response = client.generate(
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
                {"type": "image_url", "image_url": "https://..."}
            ]
        }
    ]
)
```

### 5. é”™è¯¯å¤„ç†

```python
import httpx
from src.client.llm import create_llm_client

async def safe_generate(client, messages):
    """å¸¦é”™è¯¯å¤„ç†çš„ç”Ÿæˆ"""
    try:
        return await client.agenerate(messages=messages, max_tokens=100)
    except httpx.TimeoutException:
        print("è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•ä¸­...")
        return await client.agenerate(messages=messages, max_tokens=100)
    except httpx.HTTPError as e:
        print(f"HTTPé”™è¯¯: {e}")
        return None
    except ValueError as e:
        print(f"å‚æ•°é”™è¯¯: {e}")
        return None

# æ‰¹é‡å¤„ç†ï¼Œä¸€ä¸ªå¤±è´¥ä¸å½±å“å…¶ä»–
async with create_llm_client("deepseek", "deepseek-chat") as client:
    tasks = [safe_generate(client, msg) for msg in messages_list]
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

### 6. è‡ªå®šä¹‰å‚æ•°

```python
client = create_llm_client(
    provider="deepseek",
    model_name="deepseek-chat",
    api_base="https://custom-api.com",  # è‡ªå®šä¹‰APIåœ°å€
    temperature=0.7,
    max_tokens=2000,
    top_p=0.9,
    timeout=120,  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    enable_retry=True,  # å¯ç”¨é‡è¯•
    max_retries=3,
    retry_delay=0.5
)
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### é…ç½®æ–‡ä»¶ï¼šconfig.json

```json
{
  "llm": {
    "providers": {
      "deepseek": {
        "api_base": "https://api.deepseek.com",
        "default_timeout": 120
      }
    },
    
    "presets": {
      "fast": {
        "provider": "deepseek",
        "model_name": "deepseek-chat",
        "temperature": 0.0,
        "max_tokens": 2048
      },
      "reasoning": {
        "provider": "deepseek",
        "model_name": "deepseek-chat",
        "enable_thinking": true
      }
    }
  }
}
```

### ç¯å¢ƒå˜é‡ï¼š.env

```bash
# DeepSeek (V3.2)
DEEPSEEK_API_KEY=sk-xxx
# æ¨¡å‹: deepseek-chat (éæ€è€ƒ), deepseek-reasoner (æ€è€ƒ)

# OpenAI
OPENAI_API_KEY=sk-xxx

# Gemini
GEMINI_API_KEY=xxx

# Anthropic
ANTHROPIC_API_KEY=sk-ant-xxx
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•é€‰æ‹©åŒæ­¥è¿˜æ˜¯å¼‚æ­¥ï¼Ÿ

**åŒæ­¥ï¼š** ç®€å•åœºæ™¯ï¼Œå•æ¬¡è¯·æ±‚
```python
client = create_llm_client("deepseek", "deepseek-chat")
response = client.generate(messages=[...])
```

**å¼‚æ­¥ï¼š** æ‰¹é‡å¤„ç†ï¼Œé«˜å¹¶å‘éœ€æ±‚
```python
async with create_llm_client("deepseek", "deepseek-chat") as client:
    responses = await asyncio.gather(
        client.agenerate(messages=msg1),
        client.agenerate(messages=msg2),
        ...
    )
```

### Q2: ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ˜¯å¿…é¡»çš„å—ï¼Ÿ

**ä¸æ˜¯å¿…é¡»ï¼Œä½†å¼ºçƒˆæ¨èï¼š**

```python
# ä¸´æ—¶æ¨¡å¼ï¼ˆå¯ä»¥ï¼‰
client = create_llm_client("deepseek", "deepseek-chat")
response = client.generate(messages=[...])
# ä¸´æ—¶HTTPå®¢æˆ·ç«¯è‡ªåŠ¨å…³é—­

# ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ¨èï¼‰
with create_llm_client("deepseek", "deepseek-chat") as client:
    response1 = client.generate(messages=[...])  # å¤ç”¨è¿æ¥
    response2 = client.generate(messages=[...])  # å¤ç”¨è¿æ¥
# æŒä¹…åŒ–HTTPå®¢æˆ·ç«¯è‡ªåŠ¨å…³é—­
```

### Q3: å¦‚ä½•å¤„ç†APIé…é¢é™åˆ¶ï¼Ÿ

ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘ï¼š

```python
semaphore = asyncio.Semaphore(5)  # æœ€å¤š5ä¸ªå¹¶å‘

async def limited_generate(client, msg):
    async with semaphore:
        return await client.agenerate(messages=msg)
```

### Q4: Geminiçš„API keyä¸ºä»€ä¹ˆä¸åŒï¼Ÿ

Geminiä½¿ç”¨URLå‚æ•°ä¼ é€’API keyï¼Œå·²è‡ªåŠ¨å¤„ç†ï¼š

```python
# æ— éœ€ç‰¹æ®Šå¤„ç†ï¼Œæ­£å¸¸ä½¿ç”¨å³å¯
client = create_llm_client("gemini", "gemini-1.5-pro")
response = client.generate(messages=[...])
# å†…éƒ¨è‡ªåŠ¨æ·»åŠ  ?key=xxx åˆ°URL
```

### Q5: å¦‚ä½•æŸ¥çœ‹åŸå§‹å“åº”ï¼Ÿ

```python
response = client.generate(messages=[...])
print(response.raw_response)  # åŸå§‹APIå“åº”
```

### Q6: æ”¯æŒæµå¼è¾“å‡ºå—ï¼Ÿ

å½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œåç»­ç‰ˆæœ¬ä¼šæ·»åŠ ã€‚

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [è®¾è®¡æ–‡æ¡£](../../../cursor_docs/llm/llm_client_design.md)
- [Adapterå¼€å‘æŒ‡å—](../../../cursor_docs/llm/adapters_guide.md)
- [å¿«é€Ÿå‚è€ƒ](../../../cursor_docs/llm/quick_reference.md)

---

## ğŸ¯ æœ€ä½³å®è·µ

1. âœ… **æ‰¹é‡å¤„ç†ä½¿ç”¨å¼‚æ­¥**ï¼šæ€§èƒ½æå‡100å€
2. âœ… **ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨**ï¼šå¤ç”¨è¿æ¥æ± 
3. âœ… **é™åˆ¶å¹¶å‘æ•°é‡**ï¼šé¿å…APIè¿‡è½½
4. âœ… **å¯ç”¨é”™è¯¯å¤„ç†**ï¼šä¸€ä¸ªå¤±è´¥ä¸å½±å“å…¶ä»–
5. âœ… **è®°å½•Tokenä½¿ç”¨**ï¼šæˆæœ¬æ§åˆ¶

---

**å®Œæˆæ—¶é—´**: 2026-01-05  
**ç»´æŠ¤è€…**: caixiongjiang
