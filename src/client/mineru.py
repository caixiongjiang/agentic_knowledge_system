#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""=================================================
@PROJECT_NAME: agentic_knowledge_system
@File    : mineru.py
@Author  : caixiongjiang
@Date    : 2025/12/29 15:42
@Function: 
    MinerU è¯·æ±‚å®¢æˆ·ç«¯
@Modify History:
         
@Copyrightï¼šCopyright(c) 2024-2026. All Rights Reserved
=================================================="""
from typing import Dict, List, Optional
import uuid
import base64
import requests
import asyncio
import time
import copy

from loguru import logger



class MineruClient:
    """
    MineruæœåŠ¡è¯·æ±‚å®¢æˆ·ç«¯ï¼Œç”¨äºå¤„ç†æ–‡æ¡£è§£æè¯·æ±‚
    æ”¯æŒå•æ–‡ä»¶è¯·æ±‚å’Œåˆ†é¡µå¹¶è¡Œè¯·æ±‚
    """
    
    def __init__(self, mineru_config: Dict):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        """
        self._mineru_config = mineru_config
        self._endpoint = mineru_config.get("endpoint")
        self._per_request_file_pages = mineru_config.get("per_request_file_pages")
        self._concurrency = mineru_config.get("concurrency", {})
        self._retry_config = mineru_config.get("retry_config", {})
        self._params = mineru_config.get("params", {})
        self._total_pages = 0
        self.logger = logger
    
    @staticmethod
    def _to_b64(file_bytes: bytes) -> str:
        """
        å°†æ–‡ä»¶è½¬æ¢ä¸ºbase64ç¼–ç 
        
        :param file_bytes: æ–‡ä»¶å­—èŠ‚å†…å®¹

        :return: base64ç¼–ç çš„æ–‡ä»¶å†…å®¹

        :raises Exception: æ–‡ä»¶è¯»å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            return base64.b64encode(file_bytes).decode('utf-8')
        except Exception as e:
            raise Exception(f'Error: {e}')
    
    async def parse_file(
        self, 
        file_bytes: bytes, 
        file_name: str, 
        pages_number: Optional[int] = None
    ) -> Dict:
        """
        è§£æå•ä¸ªæ–‡ä»¶
        
        :param file_bytes: æ–‡ä»¶å­—èŠ‚å†…å®¹
        :param file_name: æ–‡ä»¶å
        :param pages_number: æ–‡ä»¶çš„é¡µæ•°

        :return: è§£æç»“æœ

        :raises Exception: è¯·æ±‚å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        self._params["file_name"] = file_name.lower()
        self._total_pages = pages_number

        if pages_number is None:
            # å¦‚æœæœªä¼ å…¥æ–‡ä»¶çš„é¡µæ•°ï¼Œåˆ™ç›´æ¥ä½¿ç”¨æ•´ä¸ªæ–‡ä»¶è¿›è¡Œè¯·æ±‚
            result = self._send_request(file_bytes, **self._params)
            result = self._transform_mineru_data(result)
            return result
        else:
            # å¦‚æœä¼ å…¥äº†æ–‡ä»¶çš„é¡µæ•°ï¼Œåˆ™ä½¿ç”¨åˆ†é¡µè¯·æ±‚
            total_pages = pages_number
            result = await self._parse_file_parallel(file_bytes=file_bytes, total_pages=total_pages)
            result = self._transform_mineru_data(result)
            return result
        

    def parse_files(self,):
        """
        æ‰¹é‡è§£æå¤šä¸ªæ–‡ä»¶
        """
        pass


    
    async def _parse_file_parallel(
        self, 
        file_bytes: bytes, 
        total_pages: int
    ) -> Dict:
        """
        å¼‚æ­¥å¹¶è¡Œè§£ææ–‡ä»¶ï¼Œå°†æ–‡ä»¶æŒ‰é¡µç èŒƒå›´åˆ†å—å¤„ç†ï¼Œç„¶ååˆå¹¶ç»“æœ
        
        :param file_bytes: æ–‡ä»¶å­—èŠ‚å†…å®¹
        :param total_pages: æ–‡ä»¶æ€»é¡µæ•°
            
        :return: åˆå¹¶åçš„è§£æç»“æœ
        """
        
        # åˆ›å»ºé¡µé¢èŒƒå›´åˆ—è¡¨
        page_ranges = [(i, min(i + self._per_request_file_pages - 1, total_pages - 1)) 
                       for i in range(0, total_pages, self._per_request_file_pages)]
        self.logger.debug(f"æ–‡ä»¶ {self._params['file_name']} åˆ†é¡µè¯·æ±‚çš„é¡µç èŒƒå›´: {page_ranges}")

        # è®¾ç½®æœ€å¤§å¹¶å‘æ•°
        concurrency = self._concurrency
        max_concurrent = min(len(page_ranges), concurrency.get("max_concurrency", 12))
        
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘è¯·æ±‚æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_range(start_page, end_page, index):
            async with semaphore:
                result = await self._send_request_async(
                    file_bytes, 
                    **self._params,
                    start_page_id=str(start_page),
                    end_page_id=str(end_page),
                )
                return index, result
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼Œå¹¶ä¿å­˜åŸå§‹ç´¢å¼•
        tasks = [
            process_range(start_page, end_page, i) 
            for i, (start_page, end_page) in enumerate(page_ranges)
        ]
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results_with_index = await asyncio.gather(*tasks)
        
        # æŒ‰åŸå§‹é¡ºåºæ’åºç»“æœ
        sorted_results = [result for _, result in sorted(results_with_index, key=lambda x: x[0])]
        
        # åˆå¹¶ç»“æœ
        return self.merge_results(sorted_results)
        
    async def _send_request_async(self, file_bytes, **kwargs):
        """
        å¼‚æ­¥å‘é€è¯·æ±‚çš„åŒ…è£…æ–¹æ³•ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶
        
        :param file_bytes: æ–‡ä»¶å­—èŠ‚å†…å®¹
        :param kwargs: å…¶ä»–å‚æ•°
        :return: è§£æç»“æœ
        """
        # è·å–é‡è¯•é…ç½®
        retry_config = self._retry_config
        max_retries = retry_config.get("max_retries", 3)
        retry_delay = retry_config.get("retry_delay", 1.0)
        retry_strategy = retry_config.get("retry_strategy", "fixed")
        
        # å®ç°é‡è¯•é€»è¾‘
        attempt = 0
        last_exception = None
        
        while attempt <= max_retries:
            try:
                # æ‰§è¡Œè¯·æ±‚
                loop = asyncio.get_running_loop()
                result = await loop.run_in_executor(
                    None, 
                    lambda: self._send_request(file_bytes, **kwargs)
                )
                return result
            except Exception as e:
                attempt += 1
                last_exception = e
                
                # å¦‚æœå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ™æŠ›å‡ºæœ€åä¸€ä¸ªå¼‚å¸¸
                if attempt > max_retries:
                    break
                
                # æ ¹æ®é‡è¯•ç­–ç•¥è®¡ç®—å»¶è¿Ÿæ—¶é—´
                delay = retry_delay
                if retry_strategy == "exponential":
                    delay = retry_delay * (2 ** (attempt - 1))
                elif retry_strategy == "linear":
                    delay = retry_delay * attempt
                elif retry_strategy == "fixed":
                    delay = retry_delay
                else:
                    raise Exception(f"Invalid retry strategy: {retry_strategy}")
                
                # ç­‰å¾…åé‡è¯•
                await asyncio.sleep(delay)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åæ•è·çš„å¼‚å¸¸
        raise last_exception

    def _send_request(self, file_bytes: bytes, **kwargs):
        try:
            response = requests.post(self._endpoint, json={
                'file': self._to_b64(file_bytes),
                'kwargs': kwargs
            })

            if response.status_code == 200:
                output = response.json()
                return output
            else:
                raise Exception(response.text)
        except Exception as e:
            raise Exception(f'Error: {e}')

    def merge_results(self, results):
        """
        åˆå¹¶å¤šä¸ªè§£æç»“æœ
        
        :param results: è§£æç»“æœåˆ—è¡¨

        :return: åˆå¹¶åçš„è§£æç»“æœ
        """
        if not results:
            return {}
        
        # è·å–ç¬¬ä¸€ä¸ªç»“æœä½œä¸ºåŸºç¡€
        merged_result = results[0].copy()
        
        # å¦‚æœæœ‰md_contentå­—æ®µï¼Œåˆå¹¶
        if "md_content" in merged_result:
            md_contents = [r.get("md_content", "") for r in results]
            merged_result["md_content"] = "\n\n".join(filter(None, md_contents))
        
        # åˆå¹¶layoutä¿¡æ¯
        if "layout" in merged_result:
            layout = []
            for i, r in enumerate(results):
                if "layout" in r:
                    # layoutä¸­çš„page_idxå·²ç»è‡ªåŠ¨å¤„ç†æ­£ç¡®
                    current_layout = r.get("layout", [])
                    layout.extend(current_layout)
            
            merged_result["layout"] = layout

        # åˆå¹¶content_listä¿¡æ¯
        if "content_list" in merged_result:
            content_list = []
            for r in results:
                if "content_list" in r:
                    # content_listä¸­çš„page_idxå·²ç»è‡ªåŠ¨å¤„ç†æ­£ç¡®
                    current_content_list = r.get("content_list", [])
                    content_list.extend(current_content_list)
            merged_result["content_list"] = content_list
        
        # åˆå¹¶infoä¿¡æ¯ï¼ŒæŒ‰ç…§è¯·æ±‚çš„é¡µç èŒƒå›´é¡ºåºåˆå¹¶
        if "info" in merged_result:
            info = {"pdf_info": []}
            
            last_max_page_idx = 0

            for i, r in enumerate(results):
                if "info" in r and "pdf_info" in r["info"]:
                    
                    # è·å–å½“å‰ç»“æœä¸­çš„pdf_infoåˆ—è¡¨
                    pdf_info_list = r["info"]["pdf_info"]
                    start_idx = last_max_page_idx
                    end_idx= min(last_max_page_idx + self._per_request_file_pages, self._total_pages)
                    pages_pdf_info = pdf_info_list[start_idx: end_idx]
                    info["pdf_info"].extend(pages_pdf_info)
                    # æ›´æ–°å·²å¤„ç†çš„æ€»é¡µæ•°
                    last_max_page_idx += self._per_request_file_pages
            merged_result["info"] = info
        else:
            # å¦‚æœæ²¡æœ‰infoå­—æ®µï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„
            merged_result["info"] = {"pdf_info": []}

        # åˆå¹¶imagesä¿¡æ¯
        if "images" in merged_result:
            images = {}
            for r in results:
                if "images" in r:
                    images.update(r["images"])
            merged_result["images"] = images
        
        return merged_result
    
    def _transform_mineru_data(self, data: Dict) -> Dict:
        """
        å°†Mineruè¿”å›çš„æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        :param data: Mineruè¿”å›çš„åŸå§‹æ•°æ®
        :return: æ ‡å‡†åŒ–çš„ç»“æ„åŒ–æ•°æ®
        """
        # è¿™é‡Œéœ€è¦æ ¹æ®Mineruçš„å®é™…è¿”å›æ ¼å¼è¿›è¡Œè½¬æ¢

        info = data.get("info", {})
        content_list = data.get("content_list", [])
        md_content = data.get("md_content", "")
        images_base64 = data.get("images", {})

        try:
            struct_content = self.nest_content_by_level(info, content_list, images_base64)
        except Exception as e:
            raise Exception(f"Mineruæ•°æ®æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")

        try:
            return {
                "status": "success",
                "struct_content": struct_content,
                "content": md_content,
                "pages": len(struct_content.get("root", []))
            }
        except Exception as e:
            raise Exception(f"Mineruæ•°æ®è½¬æ¢å¤±è´¥: {str(e)}")

    @staticmethod
    def nest_content_by_level(info: Dict, content_list: List, images_base64: Dict):

        nest_data = []

        pdf_info = info.get("pdf_info", [])
        total_preproc_blocks = sum([len(page_info.get("preproc_blocks", [])) for page_info in pdf_info])
        assert total_preproc_blocks == len(content_list), \
            "preproc_blocksæ•°é‡ä¸contentæ•°é‡ä¸åŒ¹é…, preproc_blocksæ•°é‡: {}, contentæ•°é‡: {}".format(total_preproc_blocks, len(content_list))

        content_list_idx = 0  # è¿½è¸ªcontent_listçš„ç´¢å¼•
        for page_idx, page_info in enumerate(pdf_info):
            page_info_item = {
                "page_idx": page_idx,
                "page_size": {
                    "width": page_info["page_size"][0],
                    "height": page_info["page_size"][1]
                },
                "page_info": []
            }
            preproc_blocks = page_info.get("preproc_blocks", [])
            for block_idx, block_info in enumerate(preproc_blocks):
                content_item = content_list[content_list_idx]
                type = block_info.get("type")
                match type:
                    case "image":
                        content_item["id"] = str(uuid.uuid4())
                        content_item["bbox"] = block_info.get("bbox")
                        image_name = content_item.get("img_path").split("/")[-1]
                        content_item["image_base64"] = images_base64.get(image_name)
                    case _:
                        content_item["id"] = str(uuid.uuid4())
                        content_item["bbox"] = block_info.get("bbox")

                page_info_item["page_info"].append(content_item)
                content_list_idx += 1

            nest_data.append(page_info_item)

        for page_item in nest_data:
            for index, element in enumerate(page_item["page_info"]):
                element["element_index"] = index

        nested = {"root": nest_data}

        return nested
    
    def print_config(self):
        """
        æ‰“å°å½“å‰å®¢æˆ·ç«¯çš„é…ç½®ä¿¡æ¯ï¼ˆç”¨äºdebugï¼‰
        """
        import json
        print("\n" + "="*80)
        print("MineruClient å½“å‰é…ç½®")
        print("="*80)
        print(json.dumps(self._mineru_config, indent=2, ensure_ascii=False))
        print("="*80 + "\n")




class Mineru2Client:
    """
    MineruæœåŠ¡è¯·æ±‚å®¢æˆ·ç«¯ï¼ˆæ–°ç‰ˆæœ¬ - å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—æ¨¡å¼ï¼‰
    
    æ–°ç‰ˆæœ¬ç‰¹æ€§ï¼š
    - å¼‚æ­¥ä»»åŠ¡æäº¤ï¼Œç«‹å³è¿”å› task_id
    - è½®è¯¢ç­‰å¾…ä»»åŠ¡å®Œæˆ
    - æ”¯æŒåˆ†é¡µè¯·æ±‚ï¼ˆå¯æŒ‡å®šé¡µç èŒƒå›´ï¼‰
    - æ”¯æŒè·å–å®Œæ•´ç»“æ„åŒ–æ•°æ®
    """
    
    def __init__(self, mineru_config: Dict):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        Args:
            mineru_config: é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
                - endpoint: API åŸºç¡€åœ°å€ï¼Œå¦‚ "http://localhost:18000"
                - timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 600
                - poll_interval: è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 2
                - params: ä»»åŠ¡å‚æ•°ï¼Œå¦‚ backend, lang, method ç­‰
        """
        self._mineru_config = mineru_config
        self._api_base_url = mineru_config.get("endpoint")
        self._timeout = mineru_config.get("timeout", 600)
        self._poll_interval = mineru_config.get("poll_interval", 2)
        self._params = mineru_config.get("params", {})
        self.logger = logger
    
    def parse_file(
        self, 
        file_bytes: bytes, 
        file_name: str, 
        start_page_id: Optional[int] = None,
        end_page_id: Optional[int] = None
    ) -> Dict:
        """
        è§£æå•ä¸ªæ–‡ä»¶ï¼ˆæ–°ç‰ˆæœ¬ï¼šå¼‚æ­¥ä»»åŠ¡æ¨¡å¼ï¼‰
        
        æ”¯æŒåˆ†é¡µè¯·æ±‚ï¼š
        - ä¸ä¼ å‚æ•°ï¼šå¤„ç†æ•´ä¸ªæ–‡ä»¶
        - ä¼  start_page_id å’Œ end_page_idï¼šå¤„ç†æŒ‡å®šé¡µç èŒƒå›´
        - åªä¼  start_page_idï¼šä»æŒ‡å®šé¡µç å¤„ç†åˆ°æœ€åä¸€é¡µ
        
        :param file_bytes: æ–‡ä»¶å­—èŠ‚å†…å®¹
        :param file_name: æ–‡ä»¶å
        :param start_page_id: èµ·å§‹é¡µç ï¼ˆä»0å¼€å§‹ï¼‰ï¼ŒNone è¡¨ç¤ºä»ç¬¬0é¡µå¼€å§‹
        :param end_page_id: ç»“æŸé¡µç ï¼ˆåŒ…å«ï¼‰ï¼ŒNone è¡¨ç¤ºå¤„ç†åˆ°æœ€åä¸€é¡µ
        
        :return: è§£æç»“æœï¼ˆä¸æ—§ç‰ˆæœ¬æ ¼å¼å…¼å®¹ï¼‰
        
        :raises Exception: è¯·æ±‚å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        
        ç¤ºä¾‹ï¼š
            # å¤„ç†æ•´ä¸ªæ–‡ä»¶
            parse_file(file_bytes, "doc.pdf")
            
            # å¤„ç†å‰10é¡µï¼ˆ0-9ï¼‰
            parse_file(file_bytes, "doc.pdf", start_page_id=0, end_page_id=9)
            
            # å¤„ç†ç¬¬5-10é¡µ
            parse_file(file_bytes, "doc.pdf", start_page_id=5, end_page_id=10)
            
            # ä»ç¬¬20é¡µåˆ°æœ€å
            parse_file(file_bytes, "doc.pdf", start_page_id=20)
        """
        # æ„å»ºæ—¥å¿—ä¿¡æ¯
        if start_page_id is not None or end_page_id is not None:
            page_range = f"{start_page_id or 0}-{end_page_id or 'end'}"
            self.logger.info(f"ğŸ“¤ æäº¤æ–‡æ¡£è§£æä»»åŠ¡: {file_name}ï¼Œé¡µç èŒƒå›´: {page_range}")
        else:
            self.logger.info(f"ğŸ“¤ æäº¤æ–‡æ¡£è§£æä»»åŠ¡: {file_name}ï¼ˆå®Œæ•´æ–‡ä»¶ï¼‰")
        
        try:
            # æ­¥éª¤1: æäº¤ä»»åŠ¡
            task_id = self._submit_task(
                file_bytes, 
                file_name,
                start_page_id=start_page_id,
                end_page_id=end_page_id
            )
            
            # æ­¥éª¤2: ç­‰å¾…ä»»åŠ¡å®Œæˆ
            self._wait_for_completion(task_id)
            
            # æ­¥éª¤3: è·å–å®Œæ•´æ•°æ®
            full_data = self._get_task_data(task_id)
            
            # æ­¥éª¤4: æå–å¹¶æ˜ å°„ä¸ºæ—§æ ¼å¼
            mineru_format_data = self._extract_mineru_format(full_data)
            
            # æ­¥éª¤5: ä½¿ç”¨ç°æœ‰çš„è½¬æ¢é€»è¾‘
            result = self._transform_mineru_data(mineru_format_data)
            
            self.logger.info(f"âœ… æ–‡æ¡£è§£æå®Œæˆ: {file_name}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡æ¡£è§£æå¤±è´¥: {file_name}, é”™è¯¯: {e}")
            raise

    def parse_files(self, file_list: List[tuple], max_concurrent: int = 5) -> List[Dict]:
        """
        æ‰¹é‡è§£æå¤šä¸ªæ–‡ä»¶ï¼ˆå¹¶å‘æäº¤ï¼Œé¡ºåºç­‰å¾…ï¼‰
        
        :param file_list: æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (file_bytes, file_name) å…ƒç»„
        :param max_concurrent: æœ€å¤§å¹¶å‘æäº¤æ•°ï¼Œé»˜è®¤5ï¼ˆé¿å…æœåŠ¡å™¨è¿‡è½½ï¼‰
        
        :return: è§£æç»“æœåˆ—è¡¨
        
        :raises Exception: å¦‚æœä»»ä½•æ–‡ä»¶è§£æå¤±è´¥
        """
        self.logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡è§£æ {len(file_list)} ä¸ªæ–‡ä»¶")
        self.logger.info(f"âš™ï¸  æœ€å¤§å¹¶å‘æäº¤æ•°: {max_concurrent}")
        
        results = []
        task_ids = []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡æäº¤ä»»åŠ¡ï¼ˆæ§åˆ¶å¹¶å‘æ•°ï¼‰
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ç¬¬1é˜¶æ®µ: æ‰¹é‡æäº¤ä»»åŠ¡")
        self.logger.info(f"{'='*60}")
        
        for idx in range(0, len(file_list), max_concurrent):
            batch = file_list[idx:idx + max_concurrent]
            batch_size = len(batch)
            
            self.logger.info(f"\nğŸ“¤ æäº¤æ‰¹æ¬¡ {idx//max_concurrent + 1}: {batch_size} ä¸ªæ–‡ä»¶")
            
            for file_bytes, file_name in batch:
                try:
                    task_id = self._submit_task(file_bytes, file_name)
                    task_ids.append({
                        "task_id": task_id,
                        "file_name": file_name,
                        "file_bytes": file_bytes
                    })
                except Exception as e:
                    self.logger.error(f"âŒ æäº¤å¤±è´¥: {file_name}, é”™è¯¯: {e}")
                    task_ids.append({
                        "task_id": None,
                        "file_name": file_name,
                        "error": str(e)
                    })
            
            # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…ç¬é—´æäº¤è¿‡å¤šä»»åŠ¡
            if idx + max_concurrent < len(file_list):
                time.sleep(0.5)
        
        self.logger.info(f"\nâœ… å·²æäº¤ {len([t for t in task_ids if t.get('task_id')])} ä¸ªä»»åŠ¡")
        
        # ç¬¬äºŒé˜¶æ®µï¼šç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ç¬¬2é˜¶æ®µ: ç­‰å¾…ä»»åŠ¡å®Œæˆ")
        self.logger.info(f"{'='*60}\n")
        
        success_count = 0
        failed_count = 0
        
        for idx, task_info in enumerate(task_ids, 1):
            file_name = task_info["file_name"]
            task_id = task_info.get("task_id")
            
            self.logger.info(f"[{idx}/{len(task_ids)}] å¤„ç†: {file_name}")
            
            if task_id is None:
                # æäº¤é˜¶æ®µå°±å¤±è´¥äº†
                results.append({
                    "file_name": file_name,
                    "status": "failed",
                    "error": task_info.get("error", "ä»»åŠ¡æäº¤å¤±è´¥")
                })
                failed_count += 1
                continue
            
            try:
                # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                self._wait_for_completion(task_id)
                
                # è·å–ç»“æœ
                full_data = self._get_task_data(task_id)
                mineru_format_data = self._extract_mineru_format(full_data)
                result = self._transform_mineru_data(mineru_format_data)
                
                results.append({
                    "file_name": file_name,
                    "status": "success",
                    "result": result
                })
                success_count += 1
                
            except Exception as e:
                self.logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {file_name}, é”™è¯¯: {e}")
                results.append({
                    "file_name": file_name,
                    "status": "failed",
                    "error": str(e),
                    "task_id": task_id
                })
                failed_count += 1
        
        # æ€»ç»“
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ“Š æ‰¹é‡è§£æå®Œæˆ")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"âœ… æˆåŠŸ: {success_count}")
        self.logger.info(f"âŒ å¤±è´¥: {failed_count}")
        self.logger.info(f"ğŸ“ æ€»è®¡: {len(file_list)}")
        
        return results

    def _submit_task(
        self, 
        file_bytes: bytes, 
        file_name: str,
        start_page_id: Optional[int] = None,
        end_page_id: Optional[int] = None
    ) -> str:
        """
        æäº¤ä»»åŠ¡åˆ°æ–°ç‰ˆæœ¬ API
        
        :param file_bytes: æ–‡ä»¶å­—èŠ‚å†…å®¹
        :param file_name: æ–‡ä»¶å
        :param start_page_id: èµ·å§‹é¡µç ï¼ˆå¯é€‰ï¼‰
        :param end_page_id: ç»“æŸé¡µç ï¼ˆå¯é€‰ï¼‰
        
        :return: task_id
        
        :raises Exception: æäº¤å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # å‡†å¤‡æ–‡ä»¶å’Œå‚æ•°
            files = {'file': (file_name, file_bytes)}
            data = {
                'backend': self._params.get('backend', 'pipeline'),
                'lang': self._params.get('lang', 'ch'),
                'method': self._params.get('method', 'auto'),
                'formula_enable': str(self._params.get('formula_enable', True)).lower(),
                'table_enable': str(self._params.get('table_enable', True)).lower(),
                'priority': str(self._params.get('priority', 0))
            }
            
            # æ·»åŠ åˆ†é¡µå‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
            if start_page_id is not None:
                data['start_page_id'] = str(start_page_id)
            if end_page_id is not None:
                data['end_page_id'] = str(end_page_id)
            
            # æäº¤ä»»åŠ¡
            response = requests.post(
                f'{self._api_base_url}/api/v1/tasks/submit',
                files=files,
                data=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                task_id = result['task_id']
                page_info = ""
                if start_page_id is not None or end_page_id is not None:
                    page_info = f"ï¼ˆé¡µç : {start_page_id or 0}-{end_page_id or 'end'}ï¼‰"
                self.logger.info(f"âœ… ä»»åŠ¡å·²æäº¤: {task_id} {page_info}")
                return task_id
            else:
                raise Exception(f"æäº¤ä»»åŠ¡å¤±è´¥: {response.text}")
                
        except Exception as e:
            raise Exception(f"æäº¤ä»»åŠ¡é”™è¯¯: {e}")

    def _wait_for_completion(self, task_id: str):
        """
        ç­‰å¾…ä»»åŠ¡å®Œæˆ
        
        :param task_id: ä»»åŠ¡ID
        
        :raises Exception: ä»»åŠ¡å¤±è´¥æˆ–è¶…æ—¶æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        start_time = time.time()
        
        self.logger.info(f"â³ ç­‰å¾…ä»»åŠ¡å®Œæˆ: {task_id}")
        
        while True:
            # æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
            try:
                response = requests.get(
                    f'{self._api_base_url}/api/v1/tasks/{task_id}',
                    timeout=10
                )
                
                if response.status_code != 200:
                    raise Exception(f"æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€å¤±è´¥: {response.text}")
                
                result = response.json()
                status = result.get('status')
                
                if status == 'completed':
                    self.logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task_id}")
                    return
                elif status == 'failed':
                    error_msg = result.get('error_message', 'Unknown error')
                    raise Exception(f"ä»»åŠ¡å¤±è´¥: {error_msg}")
                elif status == 'cancelled':
                    raise Exception("ä»»åŠ¡å·²è¢«å–æ¶ˆ")
                
                # æ£€æŸ¥è¶…æ—¶
                elapsed_time = time.time() - start_time
                if elapsed_time > self._timeout:
                    raise Exception(f"ä»»åŠ¡è¶…æ—¶ï¼ˆ{self._timeout}ç§’ï¼‰")
                
                # ç­‰å¾…åç»§ç»­è½®è¯¢
                time.sleep(self._poll_interval)
                
            except requests.exceptions.RequestException as e:
                raise Exception(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")

    def _get_task_data(self, task_id: str) -> Dict:
        """
        è·å–ä»»åŠ¡çš„å®Œæ•´æ•°æ®
        
        :param task_id: ä»»åŠ¡ID
        
        :return: å®Œæ•´çš„ä»»åŠ¡æ•°æ®
        
        :raises Exception: è·å–æ•°æ®å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            response = requests.get(
                f'{self._api_base_url}/api/v1/tasks/{task_id}/data',
                params={
                    'include_fields': 'md,content_list,middle_json,images',
                    'upload_images': False,
                    'include_image_base64': True,  # è·å–å›¾ç‰‡çš„ base64
                    'include_metadata': False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('status') == 'completed':
                    return result
                else:
                    raise Exception(f"ä»»åŠ¡æœªå®Œæˆï¼ŒçŠ¶æ€: {result.get('status')}")
            else:
                raise Exception(f"è·å–ä»»åŠ¡æ•°æ®å¤±è´¥: {response.text}")
                
        except Exception as e:
            raise Exception(f"è·å–ä»»åŠ¡æ•°æ®é”™è¯¯: {e}")

    def _extract_mineru_format(self, full_data: Dict) -> Dict:
        """
        å°†æ–°ç‰ˆæœ¬ API æ•°æ®æå–å¹¶æ˜ å°„ä¸ºæ—§ç‰ˆæœ¬æ ¼å¼
        
        è¿™æ ·å¯ä»¥å¤ç”¨ç°æœ‰çš„ _transform_mineru_data æ–¹æ³•
        
        :param full_data: æ–°ç‰ˆæœ¬ API è¿”å›çš„å®Œæ•´æ•°æ®
        
        :return: æ—§ç‰ˆæœ¬æ ¼å¼çš„æ•°æ®å­—å…¸
        """
        data_content = full_data.get('data', {})
        
        # 1. æå– markdown å†…å®¹
        md_content = data_content.get('markdown', {}).get('content', '')
        
        # 2. æå– content_listï¼Œå¹¶è¿‡æ»¤æ‰ type="discarded" çš„å…ƒç´ 
        content_list_raw = data_content.get('content_list', {}).get('content', [])
        content_list = [item for item in content_list_raw if item.get('type') != 'discarded']
        
        # 3. ä» middle_json æå– pdf_info
        middle_json = data_content.get('middle_json', {}).get('content', {})
        pdf_info = middle_json.get('pdf_info', [])
        
        # 4. è½¬æ¢å›¾ç‰‡æ ¼å¼ï¼šä»åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ {filename: base64}
        images_list = data_content.get('images', {}).get('list', [])
        images_dict = {}
        for img in images_list:
            img_name = img.get('name')
            img_base64 = img.get('base64')
            if img_name and img_base64:
                images_dict[img_name] = img_base64
        
        # æ„å»ºæ—§ç‰ˆæœ¬æ ¼å¼
        return {
            "md_content": md_content,
            "content_list": content_list,
            "info": {"pdf_info": pdf_info},
            "images": images_dict
        }

    def _transform_mineru_data(self, data: Dict) -> Dict:
        """
        å°†Mineruè¿”å›çš„æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        
        :param data: Mineruè¿”å›çš„åŸå§‹æ•°æ®
        :return: æ ‡å‡†åŒ–çš„ç»“æ„åŒ–æ•°æ®
        """
        info = data.get("info", {})
        content_list = data.get("content_list", [])
        md_content = data.get("md_content", "")
        images_base64 = data.get("images", {})

        try:
            struct_content = self.nest_content_by_level(info, content_list, images_base64)
        except Exception as e:
            raise Exception(f"Mineruæ•°æ®æ ¼å¼è½¬æ¢å¤±è´¥: {str(e)}")

        try:
            return {
                "status": "success",
                "struct_content": struct_content,
                "content": md_content,
                "pages": len(struct_content.get("root", []))
            }
        except Exception as e:
            raise Exception(f"Mineruæ•°æ®è½¬æ¢å¤±è´¥: {str(e)}")

    @staticmethod
    def nest_content_by_level(info: Dict, content_list: List, images_base64: Dict):
        """
        å°†å†…å®¹æŒ‰é¡µé¢ç»“æ„åµŒå¥—
        
        :param info: åŒ…å« pdf_info çš„ä¿¡æ¯å­—å…¸
        :param content_list: å†…å®¹åˆ—è¡¨
        :param images_base64: å›¾ç‰‡ base64 å­—å…¸
        
        :return: åµŒå¥—ç»“æ„çš„æ•°æ®
        """
        nest_data = []

        pdf_info = info.get("pdf_info", [])
        total_preproc_blocks = sum([len(page_info.get("preproc_blocks", [])) for page_info in pdf_info])
        assert total_preproc_blocks == len(content_list), \
            f"preproc_blocksæ•°é‡ä¸contentæ•°é‡ä¸åŒ¹é…, preproc_blocksæ•°é‡: {total_preproc_blocks}, contentæ•°é‡: {len(content_list)}"

        content_list_idx = 0  # è¿½è¸ªcontent_listçš„ç´¢å¼•
        for page_idx, page_info in enumerate(pdf_info):
            page_info_item = {
                "page_idx": page_idx,
                "page_size": {
                    "width": page_info["page_size"][0],
                    "height": page_info["page_size"][1]
                },
                "page_info": []
            }
            preproc_blocks = page_info.get("preproc_blocks", [])
            for block_idx, block_info in enumerate(preproc_blocks):
                # ä½¿ç”¨æ·±æ‹·è´é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
                content_item = copy.deepcopy(content_list[content_list_idx])
                type = block_info.get("type")
                match type:
                    case "image":
                        content_item["id"] = str(uuid.uuid4())
                        content_item["bbox"] = block_info.get("bbox")
                        # ä» content_item ä¸­è·å–å›¾ç‰‡è·¯å¾„
                        img_path = content_item.get("img_path", "")
                        if img_path:
                            image_name = img_path.split("/")[-1]
                            content_item["image_base64"] = images_base64.get(image_name)
                    case _:
                        content_item["id"] = str(uuid.uuid4())
                        content_item["bbox"] = block_info.get("bbox")

                page_info_item["page_info"].append(content_item)
                content_list_idx += 1

            nest_data.append(page_info_item)

        for page_item in nest_data:
            for index, element in enumerate(page_item["page_info"]):
                element["element_index"] = index

        nested = {"root": nest_data}

        return nested
    
    def print_config(self):
        """
        æ‰“å°å½“å‰å®¢æˆ·ç«¯çš„é…ç½®ä¿¡æ¯ï¼ˆç”¨äºdebugï¼‰
        """
        import json
        print("\n" + "="*80)
        print("Mineru2Client å½“å‰é…ç½®")
        print("="*80)
        print(json.dumps(self._mineru_config, indent=2, ensure_ascii=False))
        print("="*80 + "\n")